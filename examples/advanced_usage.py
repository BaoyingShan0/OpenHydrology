#!/usr/bin/env python3
"""
é«˜çº§ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºé«˜çº§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰é…ç½®ã€è´¨é‡è¯„ä¼°ã€å•ç‹¬ä½¿ç”¨å„ç»„ä»¶ç­‰
"""

import sys
import os
import json
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from src.skills.pipeline import HydroDataPipeline
from src.skills.parser import HydroDataParser
from src.skills.cleaner import HydroDataCleaner
from src.skills.enhancer import HydroDataEnhancer
from src.skills.evaluator import HydroDataEvaluator
from src.config import config_manager
from src.utils import setup_logging


def custom_configuration_example():
    """è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹"""
    print("ğŸ”§ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹")
    print("-" * 30)
    
    # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    custom_config = {
        "global": {
            "log_level": "DEBUG",
            "output_dir": "./custom_output",
            "max_workers": 8
        },
        "parser": {
            "supported_formats": ["txt", "pdf"],
            "text_settings": {
                "chunk_size": 500,
                "overlap": 50
            }
        },
        "cleaner": {
            "remove_duplicates": True,
            "normalize_whitespace": True,
            "min_text_length": 20
        },
        "enhancer": {
            "enable_qa_generation": True,
            "enable_term_extraction": True,
            "domain_knowledge": {
                "hydrology": True,
                "engineering": True
            }
        },
        "evaluator": {
            "quality_metrics": ["completeness", "relevance", "consistency", "diversity"],
            "thresholds": {
                "min_quality_score": 0.8
            }
        },
        "pipeline": {
            "batch_size": 5,
            "parallel_processing": True,
            "checkpoint_enabled": False
        }
    }
    
    # ä¿å­˜è‡ªå®šä¹‰é…ç½®
    config_file = "examples/custom_config.yaml"
    import yaml
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(custom_config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"ğŸ“ åˆ›å»ºè‡ªå®šä¹‰é…ç½®æ–‡ä»¶: {config_file}")
    
    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
    try:
        # é‡æ–°åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        from src.config import ConfigManager
        custom_config_manager = ConfigManager(config_file)
        
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºpipeline
        pipeline = HydroDataPipeline()
        
        print("âœ… è‡ªå®šä¹‰é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   - æ‰¹å¤„ç†å¤§å°: {custom_config_manager.get('pipeline.batch_size')}")
        print(f"   - æœ€å¤§å·¥ä½œçº¿ç¨‹: {custom_config_manager.get('global.max_workers')}")
        print(f"   - æœ€ä½è´¨é‡è¯„åˆ†: {custom_config_manager.get('evaluator.thresholds.min_quality_score')}")
        
    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰é…ç½®å¤±è´¥: {e}")
    
    finally:
        # æ¸…ç†é…ç½®æ–‡ä»¶
        if os.path.exists(config_file):
            os.remove(config_file)


def individual_component_usage():
    """å•ç‹¬ä½¿ç”¨å„ç»„ä»¶ç¤ºä¾‹"""
    print("\nğŸ”§ å•ç‹¬ä½¿ç”¨å„ç»„ä»¶ç¤ºä¾‹")
    print("-" * 30)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬
    sample_text = """
    æ°´åˆ©å·¥ç¨‹æ˜¯å›½æ°‘ç»æµçš„é‡è¦åŸºç¡€è®¾æ–½ã€‚å¤§åæ˜¯æ°´åˆ©å·¥ç¨‹çš„æ ¸å¿ƒå»ºç­‘ç‰©ï¼Œä¸»è¦ç”¨äºé˜²æ´ªã€å‘ç”µã€çŒæº‰ç­‰ã€‚
    
    æ°´åº“è°ƒåº¦æ˜¯æ°´åˆ©å·¥ç¨‹ç®¡ç†çš„å…³é”®ç¯èŠ‚ã€‚é€šè¿‡ç§‘å­¦è°ƒåº¦ï¼Œå¯ä»¥å®ç°æ°´èµ„æºçš„ä¼˜åŒ–é…ç½®ã€‚
    
    ä»€ä¹ˆæ˜¯æ´ªæ°´ï¼Ÿæ´ªæ°´æ˜¯æŒ‡æ±Ÿæ²³æ¹–æ³Šæ°´é‡è¶…è¿‡æ­£å¸¸æ°´ä½çš„ç°è±¡ã€‚é˜²æ´ªæªæ–½åŒ…æ‹¬å ¤é˜²å»ºè®¾ã€æ°´åº“è°ƒåº¦ã€åˆ†æ´ªå·¥ç¨‹ç­‰ã€‚
    """
    
    from src.models import DataChunk, DataType, Language, SourceMetadata
    from datetime import datetime
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®å—
    chunk = DataChunk(
        content=sample_text.strip(),
        data_type=DataType.TEXT,
        language=Language.CHINESE
    )
    
    print(f"ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
    
    # 1. å•ç‹¬ä½¿ç”¨Parser
    print("\n1ï¸âƒ£ ä½¿ç”¨ Parserï¼ˆè§£æå™¨ï¼‰")
    try:
        parser = HydroDataParser()
        # Parserä¸»è¦ç”¨äºè§£ææ–‡ä»¶ï¼Œè¿™é‡Œæ¼”ç¤ºæ–‡ä»¶è§£æ
        temp_file = "examples/temp_sample.txt"
        Path(temp_file).parent.mkdir(parents=True, exist_ok=True)
        with open(temp_file, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        
        parsed_chunks = parser.parse_file(temp_file)
        print(f"   è§£æç»“æœ: {len(parsed_chunks)} ä¸ªæ•°æ®å—")
        print(f"   æ£€æµ‹è¯­è¨€: {parsed_chunks[0].language.value}")
        
        os.remove(temp_file)
        
    except Exception as e:
        print(f"   âŒ Parserä½¿ç”¨å¤±è´¥: {e}")
    
    # 2. å•ç‹¬ä½¿ç”¨Cleaner
    print("\n2ï¸âƒ£ ä½¿ç”¨ Cleanerï¼ˆæ¸…æ´—å™¨ï¼‰")
    try:
        cleaner = HydroDataCleaner()
        cleaned_chunk = cleaner.process_single(chunk)
        
        print(f"   æ¸…æ´—å‰é•¿åº¦: {len(chunk.content)}")
        print(f"   æ¸…æ´—åé•¿åº¦: {len(cleaned_chunk.content)}")
        print(f"   æ¸…æ´—æ¯”ç‡: {len(cleaned_chunk.content)/len(chunk.content):.3f}")
        
        if "cleaned" in cleaned_chunk.extra_data:
            print(f"   æ¸…æ´—æ ‡è®°: {cleaned_chunk.extra_data['cleaned']}")
            
    except Exception as e:
        print(f"   âŒ Cleanerä½¿ç”¨å¤±è´¥: {e}")
    
    # 3. å•ç‹¬ä½¿ç”¨Enhancer
    print("\n3ï¸âƒ£ ä½¿ç”¨ Enhancerï¼ˆå¢å¼ºå™¨ï¼‰")
    try:
        enhancer = HydroDataEnhancer()
        enhanced_chunk = enhancer.process_single(chunk)
        
        if "extracted_terms" in enhanced_chunk.extra_data:
            terms = enhanced_chunk.extra_data["extracted_terms"]
            print(f"   æå–æœ¯è¯­: {[t['term'] for t in terms[:5]]}")
        
        if "generated_qa" in enhanced_chunk.extra_data:
            qa_list = enhanced_chunk.extra_data["generated_qa"]
            print(f"   ç”Ÿæˆé—®ç­”å¯¹: {len(qa_list)} ä¸ª")
            if qa_list:
                print(f"   ç¤ºä¾‹é—®é¢˜: {qa_list[0]['question']}")
        
        if "domain_tags" in enhanced_chunk.extra_data:
            domains = enhanced_chunk.extra_data["domain_tags"]
            print(f"   é¢†åŸŸæ ‡ç­¾: {domains}")
            
    except Exception as e:
        print(f"   âŒ Enhancerä½¿ç”¨å¤±è´¥: {e}")
    
    # 4. å•ç‹¬ä½¿ç”¨Evaluator
    print("\n4ï¸âƒ£ ä½¿ç”¨ Evaluatorï¼ˆè¯„ä¼°å™¨ï¼‰")
    try:
        evaluator = HydroDataEvaluator()
        evaluated_chunk = evaluator.process_single(chunk)
        
        if "quality_score" in evaluated_chunk.extra_data:
            quality = evaluated_chunk.extra_data["quality_score"]
            print(f"   æ€»ä½“è¯„åˆ†: {quality['overall_score']:.3f}")
            print(f"   å®Œæ•´æ€§è¯„åˆ†: {quality['completeness_score']:.3f}")
            print(f"   ç›¸å…³æ€§è¯„åˆ†: {quality['relevance_score']:.3f}")
            print(f"   ä¸€è‡´æ€§è¯„åˆ†: {quality['consistency_score']:.3f}")
            print(f"   å¤šæ ·æ€§è¯„åˆ†: {quality['diversity_score']:.3f}")
        
        if "improvement_suggestions" in evaluated_chunk.extra_data:
            suggestions = evaluated_chunk.extra_data["improvement_suggestions"]
            print(f"   æ”¹è¿›å»ºè®®: {suggestions}")
            
    except Exception as e:
        print(f"   âŒ Evaluatorä½¿ç”¨å¤±è´¥: {e}")


def quality_assessment_demo():
    """è´¨é‡è¯„ä¼°æ¼”ç¤º"""
    print("\nğŸ“Š è´¨é‡è¯„ä¼°è¯¦ç»†æ¼”ç¤º")
    print("-" * 30)
    
    # åˆ›å»ºä¸åŒè´¨é‡çš„ç¤ºä¾‹æ–‡æœ¬
    quality_examples = {
        "high_quality": """
        æ°´åˆ©å·¥ç¨‹ä¸­çš„æ°´åº“è°ƒåº¦æ˜¯ä¸€ä¸ªå¤æ‚çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚ç°ä»£æ°´åº“è°ƒåº¦éœ€è¦ç»¼åˆè€ƒè™‘é˜²æ´ªã€å‘ç”µã€çŒæº‰ã€ä¾›æ°´ç­‰å¤šé‡éœ€æ±‚ã€‚
        
        ä»æŠ€æœ¯è§’åº¦çœ‹ï¼Œæ°´åº“è°ƒåº¦æ¶‰åŠæ°´æ–‡é¢„æŠ¥ã€å…¥åº“æµé‡è®¡ç®—ã€å‡ºåº“æ§åˆ¶ç­‰å¤šä¸ªç¯èŠ‚ã€‚å…¶ä¸­ï¼Œæ°´æ–‡é¢„æŠ¥ç²¾åº¦ç›´æ¥å½±å“è°ƒåº¦æ•ˆæœã€‚
        
        ç›®å‰å¸¸ç”¨çš„è°ƒåº¦æ–¹æ³•åŒ…æ‹¬è§„åˆ™è°ƒåº¦ã€ä¼˜åŒ–è°ƒåº¦å’Œæ™ºèƒ½è°ƒåº¦ä¸‰å¤§ç±»ã€‚ä¼˜åŒ–è°ƒåº¦åˆå¯åˆ†ä¸ºçº¿æ€§è§„åˆ’ã€éçº¿æ€§è§„åˆ’ã€åŠ¨æ€è§„åˆ’ç­‰æ–¹æ³•ã€‚
        
        å®è·µè¡¨æ˜ï¼Œç»“åˆäººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ™ºèƒ½è°ƒåº¦æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜è°ƒåº¦æ•ˆç‡å’Œç»æµæ•ˆç›Šã€‚ä¾‹å¦‚ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„è°ƒåº¦æ¨¡å‹åœ¨å¤šä¸ªæ°´åº“ä¸­å–å¾—äº†è‰¯å¥½æ•ˆæœã€‚
        """,
        
        "medium_quality": "æ°´åº“è°ƒåº¦å¾ˆé‡è¦ã€‚è¦è€ƒè™‘é˜²æ´ªå’Œå‘ç”µã€‚ç”¨æ°´éœ€è¦åˆç†å®‰æ’ã€‚è°ƒåº¦æ–¹æ³•æœ‰å¾ˆå¤šç§ã€‚",
        
        "low_quality": "æ°´ã€‚åº“ã€‚è°ƒã€‚åº¦ã€‚"
    }
    
    evaluator = HydroDataEvaluator()
    
    for quality_name, text in quality_examples.items():
        print(f"\nğŸ“ {quality_name.replace('_', ' ').title()}:")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        from src.models import DataChunk, DataType, Language
        chunk = DataChunk(
            content=text.strip(),
            data_type=DataType.TEXT,
            language=Language.CHINESE
        )
        
        evaluated_chunk = evaluator.process_single(chunk)
        
        if "quality_score" in evaluated_chunk.extra_data:
            quality = evaluated_chunk.extra_data["quality_score"]
            print(f"   æ€»ä½“è¯„åˆ†: {quality['overall_score']:.3f}")
            print(f"   å®Œæ•´æ€§: {quality['completeness_score']:.3f}")
            print(f"   ç›¸å…³æ€§: {quality['relevance_score']:.3f}")
            print(f"   ä¸€è‡´æ€§: {quality['consistency_score']:.3f}")
            print(f"   å¤šæ ·æ€§: {quality['diversity_score']:.3f}")


def knowledge_base_demo():
    """çŸ¥è¯†åº“æ¼”ç¤º"""
    print("\nğŸ§  çŸ¥è¯†åº“æ¼”ç¤º")
    print("-" * 30)
    
    try:
        enhancer = HydroDataEnhancer()
        knowledge_base = enhancer.get_knowledge_base()
        
        print(f"ğŸ“š çŸ¥è¯†åº“ç»Ÿè®¡:")
        print(f"   æœ¯è¯­æ•°é‡: {len(knowledge_base.terms)}")
        print(f"   å®ä½“æ•°é‡: {len(knowledge_base.entities)}")
        print(f"   å…³ç³»æ•°é‡: {len(knowledge_base.relationships)}")
        
        print(f"\nğŸ·ï¸  éƒ¨åˆ†æœ¯è¯­ç¤ºä¾‹:")
        for i, (term, aliases) in enumerate(list(knowledge_base.terms.items())[:5]):
            print(f"   {i+1}. {term}: {aliases}")
        
        print(f"\nğŸ¢ éƒ¨åˆ†å®ä½“ç¤ºä¾‹:")
        for i, (entity_id, entity_info) in enumerate(list(knowledge_base.entities.items())[:3]):
            print(f"   {i+1}. {entity_id}: {entity_info}")
        
        print(f"\nğŸ”— éƒ¨åˆ†å…³ç³»ç¤ºä¾‹:")
        for i, relation in enumerate(knowledge_base.relationships[:3]):
            print(f"   {i+1}. {relation['subject']} -> {relation['relation']} -> {relation['object']}")
        
        # æ¼”ç¤ºæ›´æ–°çŸ¥è¯†åº“
        print(f"\nâ• æ›´æ–°çŸ¥è¯†åº“æ¼”ç¤º:")
        new_terms = ["æ°´åˆ©å·¥ç¨‹", "æ°´èµ„æºç®¡ç†", "æ°´ç”Ÿæ€ä¿æŠ¤"]
        new_entities = {
            "å—æ°´åŒ—è°ƒ": {"type": "è°ƒæ°´å·¥ç¨‹", "length": "4350km"},
            "ä¸‰å³¡å¤§å": {"type": "æ··å‡åœŸé‡åŠ›å", "height": "185m"}
        }
        
        enhancer.update_knowledge_base(new_terms, new_entities)
        
        updated_kb = enhancer.get_knowledge_base()
        print(f"   æ›´æ–°åæœ¯è¯­æ•°é‡: {len(updated_kb.terms)}")
        print(f"   æ›´æ–°åå®ä½“æ•°é‡: {len(updated_kb.entities)}")
        
    except Exception as e:
        print(f"âŒ çŸ¥è¯†åº“æ¼”ç¤ºå¤±è´¥: {e}")


def performance_monitoring_demo():
    """æ€§èƒ½ç›‘æ§æ¼”ç¤º"""
    print("\nğŸ“ˆ æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    print("-" * 30)
    
    # åˆ›å»ºå¤šä¸ªç¤ºä¾‹æ–‡ä»¶
    sample_dir = Path("examples/performance_samples")
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    sample_files = []
    for i in range(5):
        file_path = sample_dir / f"sample_{i+1}.txt"
        content = f"""
        è¿™æ˜¯ç¬¬{i+1}ä¸ªç¤ºä¾‹æ–‡ä»¶ã€‚æ°´åˆ©å·¥ç¨‹ç¬¬{i+1}éƒ¨åˆ†å†…å®¹ã€‚
        
        å…³é”®æŠ€æœ¯æŒ‡æ ‡{i+1}ï¼š
        - æŒ‡æ ‡A: {i+1}0.5
        - æŒ‡æ ‡B: {i+1}2.3
        - æŒ‡æ ‡C: {i+1}8.7
        
        è¿™ä¸ªæ–‡ä»¶ç”¨äºæ¼”ç¤ºæ€§èƒ½ç›‘æ§åŠŸèƒ½ã€‚åŒ…å«{i+1}ä¸ªä¸åŒçš„æ•°æ®ç‚¹ã€‚
        """
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
        sample_files.append(str(file_path))
    
    try:
        pipeline = HydroDataPipeline()
        
        # å¤„ç†æ–‡ä»¶
        result = pipeline.process_files(
            file_paths=sample_files,
            output_path="examples/performance_result.json"
        )
        
        if result.success:
            # è·å–å¤„ç†æŠ¥å‘Š
            report = pipeline.get_processing_report()
            
            print("ğŸ“Š æ€§èƒ½ç›‘æ§æŠ¥å‘Š:")
            print(f"   æ€»æ–‡ä»¶æ•°: {report['statistics']['total_files']}")
            print(f"   æˆåŠŸå¤„ç†: {report['statistics']['processed_files']}")
            print(f"   å¤±è´¥æ–‡ä»¶: {report['statistics']['failed_files']}")
            print(f"   æ€»æ•°æ®å—: {report['statistics']['total_chunks']}")
            print(f"   å¤„ç†æ—¶é—´: {report['statistics']['total_time']:.2f}ç§’")
            
            # å„æŠ€èƒ½ç»Ÿè®¡
            print(f"\nğŸ”§ å„æŠ€èƒ½ç»Ÿè®¡:")
            for skill_name, stats in report['skill_statistics'].items():
                print(f"   {skill_name}:")
                print(f"     å¤„ç†æ¬¡æ•°: {stats.get('processed_count', 0)}")
                print(f"     å¤±è´¥æ¬¡æ•°: {stats.get('failed_count', 0)}")
                print(f"     æˆåŠŸç‡: {stats.get('success_rate', 0):.1f}%")
                print(f"     å¹³å‡å¤„ç†æ—¶é—´: {stats.get('average_processing_time', 0):.3f}ç§’")
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ç›‘æ§æ¼”ç¤ºå¤±è´¥: {e}")
    
    finally:
        # æ¸…ç†
        import shutil
        if sample_dir.exists():
            shutil.rmtree(sample_dir)


def main():
    """ä¸»å‡½æ•°"""
    setup_logging("INFO")
    
    print("=" * 60)
    print("æ°´åˆ©æ•°æ®å¤„ç†å·¥å…· - é«˜çº§ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # 1. è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹
    custom_configuration_example()
    
    # 2. å•ç‹¬ä½¿ç”¨å„ç»„ä»¶ç¤ºä¾‹
    individual_component_usage()
    
    # 3. è´¨é‡è¯„ä¼°æ¼”ç¤º
    quality_assessment_demo()
    
    # 4. çŸ¥è¯†åº“æ¼”ç¤º
    knowledge_base_demo()
    
    # 5. æ€§èƒ½ç›‘æ§æ¼”ç¤º
    performance_monitoring_demo()
    
    print("\n" + "=" * 60)
    print("é«˜çº§ä½¿ç”¨ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()