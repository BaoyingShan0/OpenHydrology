#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶å’Œç›®å½•
"""

import sys
import os
import time
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from src.skills.pipeline import HydroDataPipeline
from src.config import config_manager
from src.utils import setup_logging


def create_batch_sample_files():
    """åˆ›å»ºæ‰¹é‡å¤„ç†çš„ç¤ºä¾‹æ–‡ä»¶"""
    samples_dir = Path("examples/batch_samples")
    samples_dir.mkdir(parents=True, exist_ok=True)
    
    # ç¤ºä¾‹æ–‡ä»¶å†…å®¹
    sample_files = {
        "hydrology_basics.txt": """
æ°´æ–‡åŸºç¡€çŸ¥è¯†

æ°´æ–‡æ˜¯ç ”ç©¶æ°´çš„å„ç§ç°è±¡å’Œè§„å¾‹çš„ç§‘å­¦ã€‚ä¸»è¦åŒ…æ‹¬é™æ°´ã€è’¸å‘ã€å¾„æµã€åœ°ä¸‹æ°´ç­‰æ°´æ–‡è¦ç´ ã€‚

é™æ°´é‡æ˜¯æŒ‡åœ¨ä¸€å®šæ—¶é—´æ®µå†…é™è½åˆ°åœ°é¢çš„æ°´é‡ï¼Œé€šå¸¸ç”¨æ¯«ç±³(mm)è¡¨ç¤ºã€‚ä¸­å›½å¹´é™æ°´é‡åˆ†å¸ƒä¸å‡ï¼Œä¸œå—æ²¿æµ·åœ°åŒºå¹´é™æ°´é‡å¯è¾¾2000mmä»¥ä¸Šï¼Œè€Œè¥¿åŒ—åœ°åŒºä¸è¶³200mmã€‚

å¾„æµæ˜¯æŒ‡é™æ°´åœ¨åœ°é¢æ±‡é›†åå½¢æˆçš„æ°´æµã€‚å¾„æµé‡æ˜¯è¡¡é‡æ°´èµ„æºä¸°å¯Œç¨‹åº¦çš„é‡è¦æŒ‡æ ‡ã€‚
        """,
        
        "dam_engineering.txt": """
å¤§åå·¥ç¨‹æŠ€æœ¯

å¤§åæ˜¯æ°´åˆ©å·¥ç¨‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¸»è¦ç”¨äºé˜²æ´ªã€å‘ç”µã€çŒæº‰ç­‰ç›®çš„ã€‚

æŒ‰ç…§å»ºç­‘ææ–™åˆ†ç±»ï¼Œå¤§åå¯åˆ†ä¸ºåœŸçŸ³åã€æ··å‡åœŸåã€ç ŒçŸ³åç­‰ã€‚å…¶ä¸­æ··å‡åœŸååˆåˆ†ä¸ºé‡åŠ›åã€æ‹±åã€æ”¯å¢©åç­‰ç±»å‹ã€‚

å¤§åå®‰å…¨ç›‘æµ‹åŒ…æ‹¬å˜å½¢ç›‘æµ‹ã€æ¸—æµç›‘æµ‹ã€åº”åŠ›ç›‘æµ‹ç­‰å¤šä¸ªæ–¹é¢ã€‚
        """,
        
        "flood_control.txt": """
é˜²æ´ªå·¥ç¨‹ä½“ç³»

é˜²æ´ªæ˜¯æ°´åˆ©å·¥ç¨‹çš„é‡è¦ä»»åŠ¡ä¹‹ä¸€ã€‚é˜²æ´ªå·¥ç¨‹ä½“ç³»åŒ…æ‹¬å ¤é˜²ã€æ°´åº“ã€è“„æ»æ´ªåŒºã€åˆ†æ´ªé“ç­‰ã€‚

å ¤é˜²æ˜¯é˜²æ­¢æ´ªæ°´æ³›æ»¥çš„ä¸»è¦å·¥ç¨‹æªæ–½ï¼ŒæŒ‰ç…§ä¿æŠ¤å¯¹è±¡å¯åˆ†ä¸ºåŸå¸‚å ¤é˜²ã€å†œæ‘å ¤é˜²ç­‰ã€‚

æ°´åº“è°ƒåº¦æ˜¯é˜²æ´ªçš„é‡è¦æ‰‹æ®µï¼Œé€šè¿‡åˆç†è°ƒåº¦æ°´åº“åº“å®¹ï¼Œå¯ä»¥æœ‰æ•ˆå‰Šå‡æ´ªå³°ã€‚
        """,
        
        "water_quality.txt": """
æ°´è´¨ç›‘æµ‹ä¸ä¿æŠ¤

æ°´è´¨æ˜¯æŒ‡æ°´çš„ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©ç‰¹æ€§ã€‚æ°´è´¨ç›‘æµ‹æ˜¯æ°´ç¯å¢ƒä¿æŠ¤çš„é‡è¦åŸºç¡€ã€‚

ä¸»è¦æ°´è´¨æŒ‡æ ‡åŒ…æ‹¬pHå€¼ã€æº¶è§£æ°§ã€æ°¨æ°®ã€æ€»ç£·ã€åŒ–å­¦éœ€æ°§é‡ç­‰ã€‚

åœ°è¡¨æ°´ç¯å¢ƒè´¨é‡æ ‡å‡†å°†æ°´è´¨åˆ†ä¸ºäº”ç±»ï¼Œå…¶ä¸­Iç±»æ°´è´¨æœ€å¥½ï¼ŒVç±»æ°´è´¨æœ€å·®ã€‚
        """
    }
    
    created_files = []
    for filename, content in sample_files.items():
        file_path = samples_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())
        created_files.append(str(file_path))
        print(f"ğŸ“ åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {file_path}")
    
    return created_files


def batch_process_files():
    """æ‰¹é‡å¤„ç†æ–‡ä»¶çš„ç¤ºä¾‹"""
    # è®¾ç½®æ—¥å¿—
    setup_logging("INFO")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    print("ğŸ“ åˆ›å»ºæ‰¹é‡å¤„ç†ç¤ºä¾‹æ–‡ä»¶...")
    sample_files = create_batch_sample_files()
    
    try:
        # åˆå§‹åŒ–pipeline
        print("\nğŸš€ åˆå§‹åŒ–æ°´åˆ©æ•°æ®å¤„ç†å·¥å…·...")
        
        # é…ç½®æ‰¹é‡å¤„ç†å‚æ•°
        config_manager.set("pipeline.batch_size", 2)
        config_manager.set("pipeline.max_workers", 2)
        
        pipeline = HydroDataPipeline()
        
        # æ‰¹é‡å¤„ç†æ–‡ä»¶
        print(f"ğŸ“„ æ‰¹é‡å¤„ç† {len(sample_files)} ä¸ªæ–‡ä»¶...")
        start_time = time.time()
        
        result = pipeline.process_files(
            file_paths=sample_files,
            output_path="examples/batch_processing_result.json"
        )
        
        processing_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        if result.success:
            print("\nâœ… æ‰¹é‡å¤„ç†æˆåŠŸ!")
            print(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            print(f"ğŸ“Š å¹³å‡æ¯ä¸ªæ–‡ä»¶å¤„ç†æ—¶é—´: {processing_time/len(sample_files):.2f}ç§’")
            
            if result.data:
                stats = result.data.get_statistics()
                print(f"\nğŸ“ˆ æ‰¹é‡å¤„ç†ç»Ÿè®¡:")
                print(f"   - æ€»æ•°æ®å—æ•°é‡: {stats['total_chunks']}")
                print(f"   - æ€»é—®ç­”å¯¹æ•°é‡: {stats['total_qa_pairs']}")
                print(f"   - æ€»å­—ç¬¦æ•°: {stats['total_characters']}")
                print(f"   - æ•°æ®ç±»å‹åˆ†å¸ƒ: {stats['data_types']}")
                print(f"   - è¯­è¨€åˆ†å¸ƒ: {stats['languages']}")
                
                # æŒ‰æ–‡ä»¶ç±»å‹åˆ†æ
                print(f"\nğŸ“‹ è¯¦ç»†åˆ†æ:")
                for chunk in result.data.chunks:
                    source_name = Path(chunk.source_metadata.file_name).stem if chunk.source_metadata else "unknown"
                    print(f"   - {source_name}: {len(chunk.content)} å­—ç¬¦")
                    
                    if "quality_score" in chunk.extra_data:
                        quality = chunk.extra_data["quality_score"]
                        print(f"     è´¨é‡è¯„åˆ†: {quality['overall_score']:.3f}")
                
                # è´¨é‡è¯„ä¼°æŠ¥å‘Š
                evaluator_stats = pipeline.skills.get("evaluator")
                if evaluator_stats:
                    report = evaluator_stats.get_evaluation_report()
                    print(f"\nğŸ“Š è´¨é‡è¯„ä¼°æŠ¥å‘Š:")
                    if "overall_stats" in report:
                        overall = report["overall_stats"]
                        print(f"   - å¹³å‡è´¨é‡è¯„åˆ†: {overall['mean']:.3f}")
                        print(f"   - è´¨é‡è¯„åˆ†æ ‡å‡†å·®: {overall['std']:.3f}")
                        print(f"   - æœ€ä½è¯„åˆ†: {overall['min']:.3f}")
                        print(f"   - æœ€é«˜è¯„åˆ†: {overall['max']:.3f}")
        else:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {result.error_message}")
            
    except Exception as e:
        print(f"ğŸ’¥ æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†ç¤ºä¾‹æ–‡ä»¶
        samples_dir = Path("examples/batch_samples")
        if samples_dir.exists():
            import shutil
            shutil.rmtree(samples_dir)
            print(f"\nğŸ—‘ï¸  æ¸…ç†ç¤ºä¾‹ç›®å½•: {samples_dir}")


def process_directory_example():
    """å¤„ç†ç›®å½•çš„ç¤ºä¾‹"""
    print("\n" + "="*50)
    print("ç›®å½•å¤„ç†ç¤ºä¾‹")
    print("="*50)
    
    # ä½¿ç”¨å·²åˆ›å»ºçš„ç¤ºä¾‹ç›®å½•
    samples_dir = Path("examples/batch_samples")
    if not samples_dir.exists():
        print("âŒ ç¤ºä¾‹ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ‰¹é‡å¤„ç†ç¤ºä¾‹")
        return
    
    try:
        # åˆå§‹åŒ–pipeline
        pipeline = HydroDataPipeline()
        
        # å¤„ç†æ•´ä¸ªç›®å½•
        print(f"ğŸ“ å¤„ç†ç›®å½•: {samples_dir}")
        result = pipeline.process_directory(
            directory_path=str(samples_dir),
            recursive=False,  # ä¸é€’å½’å¤„ç†å­ç›®å½•
            output_path="examples/directory_processing_result.json"
        )
        
        if result.success:
            print("âœ… ç›®å½•å¤„ç†æˆåŠŸ!")
            if result.data:
                stats = result.data.get_statistics()
                print(f"ğŸ“Š ç›®å½•å¤„ç†ç»Ÿè®¡:")
                print(f"   - æ•°æ®å—æ•°é‡: {stats['total_chunks']}")
                print(f"   - é—®ç­”å¯¹æ•°é‡: {stats['total_qa_pairs']}")
        else:
            print(f"âŒ ç›®å½•å¤„ç†å¤±è´¥: {result.error_message}")
            
    except Exception as e:
        print(f"ğŸ’¥ ç›®å½•å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")


def parallel_processing_comparison():
    """å¹¶è¡Œå¤„ç†æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹"""
    print("\n" + "="*50)
    print("å¹¶è¡Œå¤„ç†æ€§èƒ½å¯¹æ¯”")
    print("="*50)
    
    # é‡æ–°åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    sample_files = create_batch_sample_files()
    
    try:
        # ä¸²è¡Œå¤„ç†
        print("ğŸ”„ ä¸²è¡Œå¤„ç†æµ‹è¯•...")
        config_manager.set("pipeline.max_workers", 1)
        config_manager.set("pipeline.parallel_processing", False)
        
        pipeline_serial = HydroDataPipeline()
        start_time = time.time()
        
        result_serial = pipeline_serial.process_files(sample_files)
        serial_time = time.time() - start_time
        
        if result_serial.success:
            print(f"   ä¸²è¡Œå¤„ç†æ—¶é—´: {serial_time:.2f}ç§’")
        
        # å¹¶è¡Œå¤„ç†
        print("\nğŸš€ å¹¶è¡Œå¤„ç†æµ‹è¯•...")
        config_manager.set("pipeline.max_workers", 4)
        config_manager.set("pipeline.parallel_processing", True)
        
        pipeline_parallel = HydroDataPipeline()
        start_time = time.time()
        
        result_parallel = pipeline_parallel.process_files(sample_files)
        parallel_time = time.time() - start_time
        
        if result_parallel.success:
            print(f"   å¹¶è¡Œå¤„ç†æ—¶é—´: {parallel_time:.2f}ç§’")
            
            # æ€§èƒ½å¯¹æ¯”
            if serial_time > 0:
                speedup = serial_time / parallel_time
                print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
                print(f"   - ä¸²è¡Œæ—¶é—´: {serial_time:.2f}ç§’")
                print(f"   - å¹¶è¡Œæ—¶é—´: {parallel_time:.2f}ç§’")
                print(f"   - åŠ é€Ÿæ¯”: {speedup:.2f}x")
                print(f"   - æ•ˆç‡æå‡: {(speedup-1)/speedup*100:.1f}%")
        
    except Exception as e:
        print(f"ğŸ’¥ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
    
    finally:
        # æ¸…ç†
        samples_dir = Path("examples/batch_samples")
        if samples_dir.exists():
            import shutil
            shutil.rmtree(samples_dir)


if __name__ == "__main__":
    print("=" * 60)
    print("æ°´åˆ©æ•°æ®å¤„ç†å·¥å…· - æ‰¹é‡å¤„ç†ç¤ºä¾‹")
    print("=" * 60)
    
    # 1. æ‰¹é‡æ–‡ä»¶å¤„ç†
    batch_process_files()
    
    # 2. ç›®å½•å¤„ç†ç¤ºä¾‹
    process_directory_example()
    
    # 3. å¹¶è¡Œå¤„ç†æ€§èƒ½å¯¹æ¯”
    parallel_processing_comparison()
    
    print("\n" + "=" * 60)
    print("æ‰¹é‡å¤„ç†ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("=" * 60)